{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import stellargraph as sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.layer.graphsage import MeanAggregator, AttentionalAggregator, MaxPoolingAggregator\n",
    "from stellargraph import globalvar\n",
    "from stellargraph.mapper import GraphSAGELinkGenerator\n",
    "from stellargraph.layer import GraphSAGE, link_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "MODELS_DIR = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \"models\")\n",
    "content_api = os.path.join(DATA_DIR, \"content_api\")\n",
    "\n",
    "api_extract_file = os.path.join(content_api,\"07-02-19\", \"content_json.csv.gz\")\n",
    "content_file = os.path.join(content_api,\"content.json.gz\")\n",
    "labelled_file = os.path.join(content_api,\"labelled.csv.gz\")\n",
    "edgefile = os.path.join(DATA_DIR, \"processed_network\", \"graphsage_test.csv.gz\")\n",
    "\n",
    "model_file = os.path.join(MODELS_DIR, \"graphsage.h5\")\n",
    "vectorizer_file = os.path.join(MODELS_DIR, \"vectorizer.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained `graphSAGE` and `tfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_file, custom_objects={'MeanAggregator': MeanAggregator})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = pickle.load(open(vectorizer_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform([\"this is a test sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled = pd.read_csv(labelled_file, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled.shape, labelled[labelled.publishing_app==\"publisher\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_csv(edgefile, compression='gzip', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id = {}\n",
    "counter=0\n",
    "for val in zip(edges.source.values, edges.target.values):\n",
    "    for v in val:\n",
    "        if v not in node_id.keys():\n",
    "            node_id[v] = counter\n",
    "            counter+=1\n",
    "edges['source_id'] = edges.source.map(lambda x : int(node_id[x]))\n",
    "edges['target_id'] = edges.target.map(lambda x : int(node_id[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map edge pairs and ids to original base_path/content_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_extrach = pd.read_csv(api_extract_file, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_extrach.dropna(subset=['content_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_id = dict(zip(api_extrach.url,api_extrach.content_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges['source_cid'] = edges.source.map(lambda x : url_id[x] if x in url_id.keys() else np.nan)\n",
    "edges['target_cid'] = edges.target.map(lambda x : url_id[x] if x in url_id.keys() else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missing(ids):\n",
    "    missing = 0\n",
    "    missing_list = []\n",
    "    all_i = 0 \n",
    "    set_id = set(labelled.content_id.values)\n",
    "    for id1 in ids:\n",
    "        if id1 not in set_id:\n",
    "            missing+=1\n",
    "            missing_list.append(id1)\n",
    "        all_i +=1\n",
    "    print(\"included: {} missing: {}\".format(all_i-missing, missing))\n",
    "    return missing_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_list1 = count_missing(set(edges.source_cid.values))\n",
    "missing_list2 = count_missing(set(edges.target_cid.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_list1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"5ef7560d-7631-11e4-a3cb-005056011aef\" in labelled.content_id.values\n",
    "# labelled[labelled.content_id==\"5ef7560d-7631-11e4-a3cb-005056011aef\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainstream = set(labelled[labelled.publishing_app==\"publisher\"].content_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mainstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{} out of {} mainstream cids in edgelist\".format(len(mainstream.intersection(set(edges.source_cid.values))),\n",
    "                                                  len(mainstream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize generator data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = labelled[labelled.publishing_app==\"publisher\"].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_cid = set(labelled.content_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labelled_cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_base = dict(zip(labelled.content_id, labelled.base_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_pairs = []\n",
    "basepath_pairs = []\n",
    "for v,w in itertools.product([list(mainstream)[1]],labelled_cid):\n",
    "    if v!=w:\n",
    "        cid_pairs.append((v,w))\n",
    "        basepath_pairs.append((cid_base[v], cid_base[w]))\n",
    "len(cid_pairs), len(basepath_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = pd.DataFrame({'source_cid':[s for s,_ in cid_pairs], \n",
    "                             'target_cid':[t for _,t in cid_pairs],\n",
    "                             'source_bp':[s for s,_ in basepath_pairs], \n",
    "                             'target_bp':[t for _,t in basepath_pairs],\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *TODO:* Base node_ids on base_paths, not content_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_node_id = max(node_id.values()) + 1\n",
    "for items in zip(predict_test.source_bp.values, predict_test.target_bp.values) :\n",
    "    for item in items:\n",
    "        if item not in node_id.keys():\n",
    "            node_id[item] = max_node_id\n",
    "            max_node_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test['source'] = predict_test['source_bp'].map(node_id)\n",
    "predict_test['target'] = predict_test['target_bp'].map(node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "for tup in labelled.itertuples():\n",
    "    if tup.base_path in node_id.keys():\n",
    "        text_dict[node_id[tup.base_path]] = \"{} {}\".format(tup.title, tup.description).rstrip()\n",
    "        \n",
    "text_list = [(key,value) for key,value in text_dict.items()]\n",
    "text = [(value) for key,value in text_list]\n",
    "index = [key for key,value in text_list]\n",
    "\n",
    "X = vectorizer.transform(text)\n",
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict[412]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled[labelled.content_id=='0f2e8c41-78fa-40f9-9eea-857c07bacd80'][['content_id', 'title','description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize `node_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data = pd.DataFrame(X.todense(),index=index)\n",
    "node_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test['label'] = \"go_to\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up stellargraph graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = node_data[node_data.columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_samples = [20, 10]\n",
    "\n",
    "def batched_predict(start,end):\n",
    "    G = nx.from_pandas_edgelist(predict_test[['source','target','label']][start:end], edge_attr=\"label\")\n",
    "\n",
    "    for nid, f in zip(node_data.index, node_features):\n",
    "        if nid in G.node.keys():\n",
    "            G.node[nid][globalvar.TYPE_ATTR_NAME] = \"page\"  # specify node type\n",
    "            G.node[nid][\"feature\"] = f\n",
    "\n",
    "    G_predict = sg.StellarGraph(G, node_features=\"feature\")\n",
    "\n",
    "    edge_ids_test = [(e[0],e[1]) for e in G_predict.edges()]\n",
    "\n",
    "    predict_gen = GraphSAGELinkGenerator(G_predict, batch_size, num_samples).flow(edge_ids_test)\n",
    "\n",
    "    print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    pred = model.predict_generator(predict_gen, verbose=1, workers=8, use_multiprocessing=True, \n",
    "                                   max_queue_size=100)\n",
    "\n",
    "    print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    print(max(pred))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(length, chunksize):\n",
    "    return [[i,i+chunksize] if i+chunksize < length else [i,length-1] for i in range(0,length,chunksize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for indices in chunker(predict_test.shape[0], 10000):\n",
    "    print(indices[0],indices[1])\n",
    "    predictions.extend(batched_predict(indices[0],indices[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test['pred'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_features[0]\n",
    "\n",
    "# G = nx.from_pandas_edgelist(predict_test[['source','target','label']], edge_attr=\"label\")\n",
    "\n",
    "# len(G.nodes)\n",
    "\n",
    "# for nid, f in zip(node_data.index, node_features):\n",
    "#     if nid in G.node.keys():\n",
    "#         G.node[nid][globalvar.TYPE_ATTR_NAME] = \"page\"  # specify node type\n",
    "#         G.node[nid][\"feature\"] = f\n",
    "\n",
    "# G_predict = sg.StellarGraph(G, node_features=\"feature\")\n",
    "\n",
    "# batch_size = 1\n",
    "# num_samples = [20, 10]\n",
    "\n",
    "# edge_ids_test = [(e[0],e[1]) for e in G_predict.edges()]\n",
    "# len(edge_ids_test)\n",
    "\n",
    "# edge_ids_test[0:2]\n",
    "\n",
    "# predict_gen = GraphSAGELinkGenerator(G_predict,  batch_size, num_samples).flow(edge_ids_test, [0]*len(edge_ids_test))\n",
    "\n",
    "# type(predict_gen), len(predict_gen.ids)\n",
    "\n",
    "# pred = model.predict_generator(predict_gen, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
